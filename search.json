[
  {
    "objectID": "notebooks/mcmc-examples.html",
    "href": "notebooks/mcmc-examples.html",
    "title": "MCMC sampling examples",
    "section": "",
    "text": "Goal: Explore a joint teaching approach, where I teach students both how to code in R in order to solve a real interesting problem rather than super basic stuff.\nlibrary(brms)\nWe want to sample from a distribution. We know it’s density function, but imagine we don’t have a function that generates random samples. There are many ways to do it, but here we’ll illustrate MCMC methods:\nWe’ll also take an opportunity to see how we might optimize the code."
  },
  {
    "objectID": "notebooks/mcmc-examples.html#metropolis-monte-carlo",
    "href": "notebooks/mcmc-examples.html#metropolis-monte-carlo",
    "title": "MCMC sampling examples",
    "section": "Metropolis Monte Carlo",
    "text": "Metropolis Monte Carlo\n\nVersion 1: Naive implementation\nPurposefully bad slow code, but works. We have our standard components:\n\na pdf to evaluate (f)\nan initial value\na proposal distribution\na fixed number of mcmc samples to target\n\nWe iterate with a while loop and append accepted samples to a vector x until the length of the vector matches the number of samples.\n\nillustrate function factories and what they do for us here (perhaps a previous example without them, hard-coding the functions)\n\n\nmcmc_metropolis1 &lt;- function(f, init, proposal_dist, samples) {\n  x &lt;- init\n  while (length(x) &lt; samples) {\n    x_ &lt;- proposal_dist(1, x[length(x)])\n    f1 &lt;- f(x[length(x)])\n    f2 &lt;- f(x_)\n    if (runif(1) &lt;= f2/f1) {\n      x &lt;- c(x, x_) \n    } else {\n      x &lt;- c(x, x[length(x)])\n    }\n  }\n  x\n}\n\n# TODO: check if can be done with distributional\ndist_vonmises &lt;- function(mu, kappa) {\n  function(x) brms::dvon_mises(x, mu = mu, kappa = kappa)\n}\n\ndist_norm &lt;- function(sd) {\n  function(n, x) rnorm(n, mean = x, sd = sd)\n}\n\nout &lt;- mcmc_metropolis1(\n  f = dist_vonmises(0, 10), \n  init = 0.5, \n  proposal_dist = dist_norm(sd = 0.25), \n  samples = 2000\n)\n\n\nadd multiple traces in a subsequent example\n\nhere is our mcmc trace:\n\nplot(out, type = \"l\")\n\n\n\n\n\n\n\n\nand the resulting distribution of samples, with the theoretical density on top:\n\nplot(density(out))\ncurve(dist_vonmises(0, 10)(x), add = TRUE, col = \"red\")\n\n\n\n\n\n\n\n\nit runs fast because this is a really simple problem, but we can optimize our function a lot. Here are a few different ways to code the same algorithm:\n\nmcmc_metropolis2 &lt;- function(f, init, proposal_dist, samples) {\n  out &lt;- numeric(samples)\n  f1 &lt;- f(init)\n  for (i in seq_len(samples)) {\n    out[i] &lt;- init\n    x &lt;- proposal_dist(1, init)\n    f2 &lt;- f(x)\n    if (runif(1) &lt;= f2/f1) {\n      init &lt;- x\n      f1 &lt;- f2\n    } \n  }\n  out\n}\n\nmcmc_metropolis3 &lt;- function(f, init, proposal_dist, samples) {\n  next_sample &lt;- function(current, ...) {\n    x_ &lt;- proposal_dist(1, current)\n    f1 &lt;- f(current)\n    f2 &lt;- f(x_)\n    if (runif(1) &lt;= f2/f1) x_ else current\n  }\n  Reduce(next_sample, numeric(samples-1), init = init, accumulate = TRUE)  \n}\n\nmcmc_metropolis4 &lt;- function(f, init, proposal_dist, samples) {\n  next_sample &lt;- function(current, ...) {\n    x_ &lt;- proposal_dist(1, current)\n    f1 &lt;- f(current)\n    f2 &lt;- f(x_)\n    if (runif(1) &lt;= f2/f1) x_ else current\n  }\n\n  out &lt;- vector(\"list\", samples)\n  for (i in seq.int(samples)) {\n    out[[i]] &lt;- init\n    init &lt;- next_sample(init)\n  }\n  out\n}\n\nThere are some speed differences among those, but they are all substantially better than the original.\n\nbench::mark(\n  mcmc_metropolis1(\n    f = function(x) exp(10 * cos(x)),\n    init = 0.5, \n    proposal_dist = dist_norm(sd = 0.25), \n    samples = 10000\n  ),\n  mcmc_metropolis2(\n    f = function(x) exp(10 * cos(x)),\n    init = 0.5, \n    proposal_dist = dist_norm(sd = 0.25), \n    samples = 10000\n  ),\n  mcmc_metropolis3(\n    f = function(x) exp(10 * cos(x)),\n    init = 0.5, \n    proposal_dist = dist_norm(sd = 0.25), \n    samples = 10000\n  ),  \n  mcmc_metropolis4(\n    f = function(x) exp(10 * cos(x)),\n    init = 0.5, \n    proposal_dist = dist_norm(sd = 0.25), \n    samples = 10000\n  ),  \n  check = FALSE \n)\n\nWarning: Some expressions had a GC in every iteration; so filtering is\ndisabled.\n\n\n# A tibble: 4 × 6\n  expression                            min  median `itr/sec` mem_alloc `gc/sec`\n  &lt;bch:expr&gt;                        &lt;bch:t&gt; &lt;bch:t&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n1 mcmc_metropolis1(f = function(x)… 102.7ms 120.3ms      7.66     382MB    80.4 \n2 mcmc_metropolis2(f = function(x)…  12.2ms  12.8ms     76.6      122KB     7.86\n3 mcmc_metropolis3(f = function(x)…  17.9ms  18.7ms     51.1      349KB     9.83\n4 mcmc_metropolis4(f = function(x)…  16.1ms  16.7ms     58.7      129KB     9.79\n\n\nAn alternative to the metropolis algorithm is rejection sampling:\n\nrejection_sampling &lt;- function(n, f, max_f, proposal_fun, ...) {\n  stopifnot(is.numeric(n), length(n) == 1, n &gt; 0)\n  stopifnot(is.numeric(max_f), length(max_f) == 1 | length(max_f) == n, max_f &gt; 0)\n\n  inner &lt;- function(n, f, max_f, proposal_fun, ..., acc = c()) {\n    if (length(acc) &gt; n) {\n      return(acc[seq_len(n)])\n    }\n    x &lt;- proposal_fun(n)\n    y &lt;- stats::runif(n) * max_f\n    accept &lt;- y &lt; f(x, ...)\n    inner(n, f, max_f, proposal_fun, ..., acc = c(acc, x[accept]))\n  }\n\n  inner(n, f, max_f, proposal_fun, ...)\n}"
  },
  {
    "objectID": "notebooks/apollonian-circles.html",
    "href": "notebooks/apollonian-circles.html",
    "title": "aRt-o Pollo",
    "section": "",
    "text": "Imagine a universe stranger than ours in which light has a peculiar property. Rather than diffusing its intensity with distance, and simply increasing it with multiple sources, light in this universe is different. When two light beams meet, their combined intensity depends on the angle between them. Right angles are the best - when two light beams meet at a point perpendicularly, they have the highest possible intensity at that point. As the angle becomes smaller or bigger than 90 degrees (pi/2 radians), the intensity becomes weaker.\nWhy would you imagine such a thing? I don’t know why I wonder about such things, but I often do. As an example, I drew the image below while thinking about this - we have two “suns”, the red dots are 90 degree intersections, the blue points are intersections with a smaller angle.\n\nDrawing can only get me so far, but this is an easy enough simulation to do in R, and the results are pretty, especially when we change some of those initial assumptions.\n\nlibrary(ggplot2)\nlibrary(purrr)\nlibrary(patchwork)\nlibrary(ambient)\nlibrary(wesanderson)\nlibrary(httpgd)\nlibrary(ggdark)\nlibrary(viridis)\nlibrary(fields)\nset.seed(12345)\n\ntheme_set(\n  dark_theme_void() +\n    theme(plot.background = element_rect(fill = \"#1F1F1F\", color = \"#1F1F1F\"))\n)\n\nWhat’s things do we need for the simplest possible simulation? - two fixed points A and B for our light sources - a way to calculate the angle formed at any other point X where beams from A and B intersect - a way to convert this angle into an intensity value - calculate the intensity for many different points in the plane - visualize the result\nHere are some basic functions to do this:\n\n#' Calculates the angle formed by AX and BX\n#'\n#' @param X A point object representing the origin point.\n#' @param A A point object representing the first vector.\n#' @param B A point object representing the second vector.\n#' @return The angle in radians between vectors A and B with respect to point X.\nangle &lt;- function(X, A, B) {\n  # restructure for efficiency with many points\n  X &lt;- as.matrix(X)\n  if (length(X) == 2) X &lt;- t(X)\n  A &lt;- rep(A, each = length(X)/2)\n  B &lt;- rep(B, each = length(X)/2)\n\n  v1 &lt;- X - A\n  v2 &lt;- X - B\n  dot_product &lt;- rowSums(v1 * v2)\n  norm &lt;- (sqrt(rowSums(v1 * v1)) * sqrt(rowSums(v2 * v2)))\n  cos_theta &lt;- dot_product / norm\n  acos(cos_theta)\n}\n\n#' Convert angle to intensity\n#'\n#' @param theta A numeric value representing the angle in radians.\n#' @return A numeric value representing the intensity.\nangle_to_intensity &lt;- function(theta) {\n  1 - abs(theta - pi / 2) / (pi / 2)\n}\n\n\n#' Calculate intensity at a point\n#' \n#' @param x A point object where the intensity is to be calculated.\n#' @param source1 A point object representing the first source point.\n#' @param source2 A point object representing the second source point.\n#' @param angle_transform_fun A function that takes an angle in radians and returns\n#'  a value to be plotted as the coordinate intensity. Default is a linear function of \n#'  the absolute deviation from a right angle.\n#' @return A numeric value in [0, 1] representing the intensity at point x.\nintensity &lt;- function(x, source1, source2, angle_transform_fun = angle_to_intensity) {\n  theta &lt;- angle(x, source1, source2)\n  angle_transform_fun(theta)\n}\n\nWith these functions in hand we can play around. We need two sources:\n\nA &lt;- c(-1, 0)\nB &lt;- c(1, 0)\n\nJust as an illustration, the maximum intensity should at a point (0, 1) which forms a right angle. The minimum intensity should be any point on the AB line, e.g. (0, 0) as it forms a 180 degree angle\n\nintensity(c(0, 1), A, B)\n\n[1] 1\n\nintensity(c(0, 0), A, B)\n\n[1] 0\n\n\nNow we need a grid of points where their beams will intersect:\n\ngrid &lt;- expand.grid(\n  x = seq(-2, 2, 0.01),\n  y = seq(-2, 2, 0.01)\n)\n\nand evaluate their intensity:\n\ngrid$intensity1 &lt;- intensity(grid[c(\"x\", \"y\")], A, B)\n\nmain &lt;- ggplot(grid, aes(x, y, fill = intensity1, color = intensity1)) +\n  geom_raster()\n  \nmain + scale_fill_gradientn(colors = viridis::inferno(256))\n\n\n\n\n\n\n\n\nThat’s kinda cool. Mostly what I expected, but it’s pretty. We can try a few more pallettes and plot settings. To make things a bit easier, I want to plot multiple granularities with each palette:\n\nplot3s &lt;- function(main, pallette) {\n  p1 &lt;- main + \n    scale_fill_stepsn(colors = pallette, n.breaks = 12) + \n    theme(legend.position = \"none\")\n  p2 &lt;- main + scale_fill_stepsn(colors = pallette, n.breaks = 24) + theme(legend.position = \"none\")\n  p3 &lt;- main + scale_fill_gradientn(colors = pallette) + theme(legend.position = \"none\")\n  p1 + p2 + p3\n}\n\nFor example with out original palette we get:\n\nplot3s(main, viridis::inferno(256))\n\n\n\n\n\n\n\n\nWith the binned version on the left I notice something that wasn’t as obvious in the smooth one: There is more than one circle! In fact, all points that form the same angle with A and B lie on a circle. That this would be the case was not at all obvious to me, though eventually I remembered that this is just the inscribed angle theorem from high school geometry. The theorem is a more general case of the more widely known Tales theorem - that all points on a circle form 90 degree angles with any diameter line of the circle. The resulting figure also represents one half of what is known as the Apollonian circles. A well deserved wikipedia rabbit-whole lead to me read more about alternative coordinate systems like the bipolar coordinate system, which at first glance seems esoteric and pointless, but turns out it can simplify many problems that are otherwise too complicated to compute in standard cartesian coordinate systems.\nAnyway, let’s go on with making pretty variations on this theme.\n\nplot3s(main, hcl.colors(12, \"YlOrRd\", rev = TRUE))\nplot3s(main, c(\"#00FFFF\", \"#8A2BE2\", \"#FFD700\"))\nplot3s(main, wes_palette(\"Royal1\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlright, how about we add some correlated noise for variety?\n\nsmooth_matrix_gaussian &lt;- function(mat, sigma = 1) {\n  # Create 1D Gaussian kernel\n  create_gaussian_kernel &lt;- function(sigma, size = NULL) {\n    if (is.null(size)) {\n      size &lt;- ceiling(3 * sigma) * 2 + 1 # Ensures kernel covers ~99% of distribution\n    }\n    x &lt;- seq(-size %/% 2, size %/% 2)\n    kernel &lt;- exp(-(x^2) / (2 * sigma^2))\n    kernel / sum(kernel) # Normalize\n  }\n\n  # Generate 1D Gaussian kernel\n  kernel_1d &lt;- create_gaussian_kernel(sigma)\n\n  # Apply separable convolution (first along rows, then columns)\n  smooth_rows &lt;- t(apply(mat, 1, function(row) convolve(row, kernel_1d, type = \"filter\")))\n  apply(smooth_rows, 2, function(col) convolve(col, kernel_1d, type = \"filter\"))\n}\n\n# padding because the smoothing kernel drops edges\ngrid_size &lt;- sqrt(nrow(grid)) + 61 \nnoise &lt;- matrix(rnorm(grid_size^2), grid_size)\nnoise &lt;- smooth_matrix_gaussian(noise, sigma = 10)\n\nNow won’t you look at that!\n\ngrid$intensity2 &lt;- grid$intensity1 + as.vector(t(noise))\n\nnoise_plot &lt;- grid |&gt; \n  ggplot(aes(x, y, fill = intensity2, color = intensity2)) +\n  geom_raster() +\n  theme(legend.position = \"none\")\n\nplot3s(noise_plot, rev(viridis::inferno(256)))\nplot3s(noise_plot, hcl.colors(12, \"YlOrRd\", rev = TRUE))\nplot3s(noise_plot, c(\"#00FFFF\", \"#8A2BE2\", \"#FFD700\"))\nplot3s(noise_plot, wes_palette(\"Royal1\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlright, one last one. Instead of clamping the intensity to be a linear function of how much the angle deviates from 90 degrees, let’s introduce some oscillations:\n\nnonlinear_intensity &lt;- function(theta) {\n  deviation &lt;- abs(theta - pi/2) / (pi/2)\n  return(sin(2 * pi * deviation)^2)  # Nonlinear transformation\n}\n\nbigger_grid &lt;- expand.grid(\n  x = seq(-4, 4, 0.02),\n  y = seq(-4, 4, 0.02)\n)\n\n# padding because the smoothing kernel drops edges\nbigger_grid_size &lt;- sqrt(nrow(bigger_grid)) + 61 \nbigger_noise &lt;- matrix(rnorm(grid_size^2, sd = 2), bigger_grid_size)\nbigger_noise &lt;- smooth_matrix_gaussian(bigger_noise, sigma = 10)\n\nbigger_grid$intensity3 &lt;- intensity(bigger_grid[c(1,2)], A, B, nonlinear_intensity) + as.vector(t(bigger_noise))\n\nnoise_plot_nl &lt;- bigger_grid |&gt; \n  ggplot(aes(y, x, fill = intensity3, color = intensity3)) +\n  geom_raster() +\n  theme(legend.position = \"none\")\n\nplot3s(noise_plot_nl, viridis::inferno(256))\nplot3s(noise_plot_nl, hcl.colors(12, \"YlOrRd\", rev = TRUE))\nplot3s(noise_plot_nl, c(\"#00FFFF\", \"#8A2BE2\", \"#FFD700\"))\nplot3s(noise_plot_nl, grey.colors(2))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLast for real this time:\n\nstreak_noise &lt;- function(mat, sigma_x = 10, sigma_y = 3) {\n  size_x &lt;- ceiling(3 * sigma_x) * 2 + 1\n  size_y &lt;- ceiling(3 * sigma_y) * 2 + 1\n  \n  kernel &lt;- outer(-size_x:size_x, -size_y:size_y, function(x, y) {\n    exp(-(x^2 / (2 * sigma_x^2) + y^2 / (2 * sigma_y^2)))\n  })\n  kernel &lt;- kernel / sum(kernel)  # Normalize kernel\n  \n  # Pad kernel to match matrix size\n  pad_kernel &lt;- matrix(0, nrow = nrow(mat), ncol = ncol(mat))\n  kx &lt;- floor(nrow(kernel) / 2)\n  ky &lt;- floor(ncol(kernel) / 2)\n  pad_kernel[1:nrow(kernel), 1:ncol(kernel)] &lt;- kernel\n  \n  convolve2d &lt;- function(mat, kernel) {\n    fft_mat &lt;- fft(mat)\n    fft_kernel &lt;- fft(kernel, dim(mat))\n    return(Re(fft(fft_mat * fft_kernel, inverse = TRUE) / length(mat)))\n  }\n  \n  return(convolve2d(mat, pad_kernel))\n}\n\n# Generate and smooth directional noise\ngrid_size &lt;- sqrt(nrow(grid))\nnoise_matrix &lt;- matrix(rnorm(grid_size^2), nrow = grid_size)\nsmoothed_noise &lt;- streak_noise(noise_matrix, sigma_x = 10, sigma_y = 2)\n\n# Normalize and scale noise\nsmoothed_noise &lt;- (smoothed_noise - min(smoothed_noise)) / \n                  (max(smoothed_noise) - min(smoothed_noise)) * 0.3 - 0.15\n\n# Apply noise and clip values\nlower_part &lt;- grid$y &lt; 0\ngrid$intensity4 &lt;- ifelse(\n  lower_part,\n  pmax(pmin(grid$intensity1^0.75 + as.vector(smoothed_noise), 1), 0),\n  grid$intensity1\n)\ninside_upper_circle &lt;- (grid$y^2+grid$x^2 &lt;= 1) & (grid$y &gt;= 0 )\ngrid$intensity4[inside_upper_circle] &lt;- grid$intensity4[inside_upper_circle]^0.4\n\nsterak_noise &lt;- grid |&gt; \n  ggplot(aes(x, y, fill = intensity4, color = intensity4)) +\n  geom_raster() +\n  theme(legend.position = \"none\")\n\nplot3s(sterak_noise, viridis::inferno(256))"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ven’s etudes",
    "section": "",
    "text": "Everything here should be considered to be in “perpetual rough draft mode” and taken with a big grain of salt. In contrast to my blog, the notebooks here are not aimed at anyone in particular aside from myself. The name etudes was inspired by Peter Norvig’s pytudes GitHub repo.\nThese are projects large and small, in which I explore new skills or interesting problems; projects that I want to preserve more than my messy ‘sandbox’ folder with various scripts, but that are not ready or intended for any type of spotlight. Some of them might eventually make it over to my blog, research projects, course materials, but most will probably enjoy the solitude and judgement-free zone that this little corner provides.\n\n\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Notebook\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nNotebook\n\n\nDate\n\n\nCategories\n\n\n\n\n\n\naRt-o Pollo\n\n\n \n\n\n \n\n\n\n\nMCMC sampling examples\n\n\n \n\n\n \n\n\n\n\nA little math can go a long way\n\n\nJan 15, 2024\n\n\nprogramming, optimizing-code\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notebooks/optimization_exercise.html",
    "href": "notebooks/optimization_exercise.html",
    "title": "A little math can go a long way",
    "section": "",
    "text": "I’m exploring the following problem as a potential exercise for students in my Scientific Computing course:\nUnit tests\ntest &lt;- function(.f) {\n  structure(\n    list(\n      tinytest::expect_equal(.f(3), 3, info = \"Input: N = 3\"),\n      tinytest::expect_equal(.f(-100), 0, info = \"Input: N = -100\"),\n      tinytest::expect_equal(.f(20), 98, info = \"Input: N = 20\"),\n      tinytest::expect_equal(.f(6), 14, info = \"Input: N = 6\"),\n      tinytest::expect_equal(.f(0), 0, info = \"Input: N = 0\")\n    ),\n    class = \"tinytests\"\n  )\n}"
  },
  {
    "objectID": "notebooks/optimization_exercise.html#code-smell",
    "href": "notebooks/optimization_exercise.html#code-smell",
    "title": "A little math can go a long way",
    "section": "Code smell",
    "text": "Code smell\nHere is a horrible way to implement this in R:\n\nsum_multiples_loop &lt;- function(N) {\n  out &lt;- 0\n  for (i in 1:N) {\n    if (i %% 3 == 0 || i %% 5 == 0) {\n      out &lt;- out + i\n    }\n  }\n  return(out)\n}\n\nsum_multiples_loop(20)\n\n[1] 98\n\n\nIt works, but there are a ton of problems that make this code really slow and we can make it several thousand times faster by the end of this post. But it is also a good idea to test more than one case, and since I don’t want to copy paste calls to each function for many different scenarios, I have written a small suite of unit tests:\n\ntest(sum_multiples_loop)\n\n----- FAILED[data]: &lt;--&gt;\n call| test(sum_multiples_loop)\n diff| Expected '0', got '-2418'\n info| Input: N = -100\n \nShowing 1 out of 5 results: 1 fails, 4 passes fubar!\n\n\nNot surprisingly, our function fails when the input is less than 0, so this is something that we need to address later."
  },
  {
    "objectID": "notebooks/optimization_exercise.html#standard-vectorization",
    "href": "notebooks/optimization_exercise.html#standard-vectorization",
    "title": "A little math can go a long way",
    "section": "Standard vectorization",
    "text": "Standard vectorization\nWe can of course vectorize a lot and simplify the whole thing dramatically by using standard R vectorization. Instead of looping over positive integers from 1 to N, we create a vector x which stores those integers. Then we take advantage some vectorizations\n\nsum_multiples_vectorized &lt;- function(N) {\n1  if (N &lt;= 0) return(0)\n2  x &lt;- seq_len(N)\n3  is_divisible &lt;- x %% 3 == 0 | x %% 5 == 0\n4  sum(x[is_divisible])\n}\n\ntest(sum_multiples_vectorized)\n\n\n1\n\nTake care of edge cases with an early return; alternative is to give an error\n\n2\n\nseq_len(N) is a safer alternative to 1:N (doesn’t matter here because we return early if N is not-positive, but good practice in general)\n\n3\n\nBoolean operations on vectors produce vectors of TRUE and FALSE values, so no need for loops and if statements\n\n4\n\nSubseting a vector with a boolean vector will return a new vector with only those values for which is_divisible is TRUE; sum is vectorized and will return the sum of all elements in x\n\n\n\n\nAll ok, 5 results fubar!\n\n\nWe can now compare the performance of the two functions. There are many ways to do this. Here is one popular option:\n\nbench::mark(\n  sum_multiples_loop(1e6),\n  sum_multiples_vectorized(1e6)\n)\n\nWarning: Some expressions had a GC in every iteration; so filtering is\ndisabled.\n\n# A tibble: 2 × 6\n  expression                           min   median `itr/sec` mem_alloc `gc/sec`\n  &lt;bch:expr&gt;                      &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n1 sum_multiples_loop(1e+06)        108.9ms  112.4ms      8.85    6.85KB    104. \n2 sum_multiples_vectorized(1e+06)   11.9ms   12.8ms     69.5     37.9MB     87.3\n\n\nbut we can do the math and get much better results:\n\nsum_multiples_math &lt;- function(N) {\n  if (N &lt;= 0) {\n    return(0)\n  }\n  triangular_number &lt;- function(n) {\n    n * (n + 1) / 2\n  }\n\n  a &lt;- 3\n  b &lt;- 5\n  c &lt;- 15\n\n  a * triangular_number(floor(N / a)) +\n    b * triangular_number(floor(N / b)) -\n    c * triangular_number(floor(N / c))\n}\n\ntest(sum_multiples_math)\n\nAll ok, 5 results fubar!\n\n\ncompare to the other two functions:\n\nbench::mark(\n  sum_multiples_loop(1e6),\n  sum_multiples_vectorized(1e6),\n  sum_multiples_math(1e6)\n)\n\nWarning: Some expressions had a GC in every iteration; so filtering is\ndisabled.\n\n# A tibble: 3 × 6\n  expression                           min   median `itr/sec` mem_alloc `gc/sec`\n  &lt;bch:expr&gt;                      &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n1 sum_multiples_loop(1e+06)        111.7ms  113.3ms    8.74e0    6.85KB    101. \n2 sum_multiples_vectorized(1e+06)   11.9ms   12.9ms    6.84e1    37.9MB     86.0\n3 sum_multiples_math(1e+06)          615ns    697ns    1.36e6   11.52KB      0  \n\n\nof course, in all cases we can parameterize the function rather than hard-coding it. The challenge would be to find the least common multiple. You could use the function pracma::Lcm\n\nsum_series_divisors &lt;- function(N, a, b) {\n  if (N &lt;= 0) {\n    return(0)\n  }\n  triangular_number &lt;- function(n) {\n    n * (n + 1) / 2\n  }\n\n  lcm &lt;- pracma::Lcm(a, b)\n\n  a * triangular_number(floor(N / a)) +\n    b * triangular_number(floor(N / b)) -\n    lcm * triangular_number(floor(N / lcm))\n}\n\ntest(\\(x) sum_series_divisors(x, a = 3, b = 5))\n\nAll ok, 5 results fubar!\n\n\nwhich incurs a cost of course, but that cost is minuscule relative to the other approaches above:\n\nbench::mark(\n  sum_multiples_math(1e6),\n  sum_series_divisors(1e6, a = 3, b = 5)\n)\n# A tibble: 2 × 6\n  expression                            min  median `itr/sec` mem_alloc `gc/sec`\n  &lt;bch:expr&gt;                        &lt;bch:t&gt; &lt;bch:t&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n1 sum_multiples_math(1e+06)           615ns   697ns  1326018.    11.5KB      0  \n2 sum_series_divisors(1e+06, a = 3…   5.7µs   6.4µs   151825.   432.8KB     60.8\n\n\nbut let’s not use any libraries and write our own lcm function…"
  }
]